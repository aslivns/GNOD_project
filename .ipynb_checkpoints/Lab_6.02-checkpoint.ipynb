{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9583bd95",
   "metadata": {},
   "source": [
    "### FBI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762732c1",
   "metadata": {},
   "source": [
    "Create a Python list with the top ten FBI's Most Wanted names: url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "275e24b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40909a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9798bb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "response.status_code # 200 status code means OK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1be72ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95b0676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.select('#query-results-0f737222c5054a81a120bce207b0446a > ul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da01d0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://www.fbi.gov/wanted/topten/yulan-adonay-archaga-carias\">YULAN ADONAY ARCHAGA CARIAS</a>,\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/bhadreshkumar-chetanbhai-patel\">BHADRESHKUMAR CHETANBHAI PATEL</a>,\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/alejandro-castillo\">ALEJANDRO ROSALES CASTILLO</a>,\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/ruja-ignatova\">RUJA IGNATOVA</a>,\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/arnoldo-jimenez\">ARNOLDO JIMENEZ</a>,\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/omar-alexander-cardenas\">OMAR ALEXANDER CARDENAS</a>,\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/alexis-flores\">ALEXIS FLORES</a>,\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/jose-rodolfo-villarreal-hernandez\">JOSE RODOLFO VILLARREAL-HERNANDEZ</a>,\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/michael-james-pratt\">MICHAEL JAMES PRATT</a>,\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/rafael-caro-quintero\">RAFAEL CARO-QUINTERO</a>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select(\"h3 > a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f4e7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "\n",
    "num_iter = len(soup.select(\"h3 > a\"))\n",
    "\n",
    "names_list = soup.select(\"h3 > a\")\n",
    "\n",
    "for i in range(num_iter):\n",
    "    names.append(names_list[i].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e79fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_wanted = pd.DataFrame({'names':names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99ed2a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YULAN ADONAY ARCHAGA CARIAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BHADRESHKUMAR CHETANBHAI PATEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALEJANDRO ROSALES CASTILLO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RUJA IGNATOVA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARNOLDO JIMENEZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OMAR ALEXANDER CARDENAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ALEXIS FLORES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JOSE RODOLFO VILLARREAL-HERNANDEZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MICHAEL JAMES PRATT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RAFAEL CARO-QUINTERO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               names\n",
       "0        YULAN ADONAY ARCHAGA CARIAS\n",
       "1     BHADRESHKUMAR CHETANBHAI PATEL\n",
       "2         ALEJANDRO ROSALES CASTILLO\n",
       "3                      RUJA IGNATOVA\n",
       "4                    ARNOLDO JIMENEZ\n",
       "5            OMAR ALEXANDER CARDENAS\n",
       "6                      ALEXIS FLORES\n",
       "7  JOSE RODOLFO VILLARREAL-HERNANDEZ\n",
       "8                MICHAEL JAMES PRATT\n",
       "9               RAFAEL CARO-QUINTERO"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_wanted.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a14df5",
   "metadata": {},
   "source": [
    "### Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbca59e",
   "metadata": {},
   "source": [
    "Display the top 10 languages by number of native speakers stored in a pandas dataframe: url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "390d0d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = \"https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dac2539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2 = requests.get(url2)\n",
    "response2.status_code # 200 status code means OK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0705d6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(response2.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "323bf3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #languages\n",
    "# soup2.select(\"tbody > tr > td > a.mw-redirect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dd414b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "language=soup2.find('table',{'class':\"wikitable\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "073e3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages=pd.read_html(str(language))\n",
    "# convert list to dataframe\n",
    "languages=pd.DataFrame(languages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3dedaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Native speakers (millions)</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese (incl. Standard Chinese, but ...</td>\n",
       "      <td>920.0</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>475.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>373.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi (excl. Urdu)</td>\n",
       "      <td>344.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>234.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>232.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russian</td>\n",
       "      <td>154.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>125.0</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yue Chinese (incl. Cantonese)</td>\n",
       "      <td>85.2</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>84.6</td>\n",
       "      <td>Austroasiatic</td>\n",
       "      <td>Vietic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Language  \\\n",
       "0  Mandarin Chinese (incl. Standard Chinese, but ...   \n",
       "1                                            Spanish   \n",
       "2                                            English   \n",
       "3                                 Hindi (excl. Urdu)   \n",
       "4                                            Bengali   \n",
       "5                                         Portuguese   \n",
       "6                                            Russian   \n",
       "7                                           Japanese   \n",
       "8                      Yue Chinese (incl. Cantonese)   \n",
       "9                                         Vietnamese   \n",
       "\n",
       "   Native speakers (millions) Language family        Branch  \n",
       "0                       920.0    Sino-Tibetan       Sinitic  \n",
       "1                       475.0   Indo-European       Romance  \n",
       "2                       373.0   Indo-European      Germanic  \n",
       "3                       344.0   Indo-European    Indo-Aryan  \n",
       "4                       234.0   Indo-European    Indo-Aryan  \n",
       "5                       232.0   Indo-European       Romance  \n",
       "6                       154.0   Indo-European  Balto-Slavic  \n",
       "7                       125.0         Japonic      Japanese  \n",
       "8                        85.2    Sino-Tibetan       Sinitic  \n",
       "9                        84.6   Austroasiatic        Vietic  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7193aca4",
   "metadata": {},
   "source": [
    "## Earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56a20f",
   "metadata": {},
   "source": [
    "Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe: url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec93f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "url3 = \"https://www.emsc-csem.org/Earthquake/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8ac4d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response3 = requests.get(url3)\n",
    "response3.status_code # 200 status code means OK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a468167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3 = BeautifulSoup(response3.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54fbc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup3.select('td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "712e89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = soup3.find_all('td',{'class','tabev6'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a5b6add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'earthquake2023-01-10\\xa0\\xa0\\xa007:15:11.919min ago'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.find_all('td',{'class','tabev6'})[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712e13b7",
   "metadata": {},
   "source": [
    "Under tabev6 > class= there are 2 times tabev2 and times tabev1. So I found a way, but I takes too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "521caa2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td class=\"tabev1\">35.54 </td>,\n",
       " <td class=\"tabev2\">N  </td>,\n",
       " <td class=\"tabev1\">25.78 </td>,\n",
       " <td class=\"tabev2\">E  </td>,\n",
       " <td class=\"tabev3\">7</td>,\n",
       " <td class=\"tabev5\" id=\"magtyp0\">ML</td>,\n",
       " <td class=\"tabev2\">2.1</td>,\n",
       " <td class=\"tb_region\" id=\"reg0\"> CRETE, GREECE</td>,\n",
       " <td class=\"comment updatetimeno\" id=\"upd0\" style=\"text-align:right;\">2023-01-10 07:30</td>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio[0].findNextSiblings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "727aba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69b1ff2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td class=\"tabev1\">35.54 </td>,\n",
       " <td class=\"tabev2\">N  </td>,\n",
       " <td class=\"tabev1\">25.78 </td>,\n",
       " <td class=\"tabev2\">E  </td>,\n",
       " <td class=\"tabev3\">7</td>,\n",
       " <td class=\"tabev5\" id=\"magtyp0\">ML</td>,\n",
       " <td class=\"tabev2\">2.1</td>,\n",
       " <td class=\"tb_region\" id=\"reg0\"> CRETE, GREECE</td>,\n",
       " <td class=\"comment updatetimeno\" id=\"upd0\" style=\"text-align:right;\">2023-01-10 07:30</td>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroth_ratio_sibs = ratio[0].findNextSiblings()\n",
    "zeroth_ratio_sibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79a67416",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_list = list(zeroth_ratio_sibs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5f3282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_direction_list = list(zeroth_ratio_sibs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81bbea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "longitude_list = list(zeroth_ratio_sibs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06ab6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "longitude_direction_list = list(zeroth_ratio_sibs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc84301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_list = list(zeroth_ratio_sibs[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56aeddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_list = list(zeroth_ratio_sibs[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73aa2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = []\n",
    "latitude = []\n",
    "latitude_direction = []\n",
    "longitude = []\n",
    "longitude_direction = []\n",
    "magnitude = []\n",
    "region = []\n",
    "\n",
    "for i in range(0,20):\n",
    "    zeroth_ratio_sibs = ratio[i].findNextSiblings()\n",
    "    date_time.append(soup3.find_all('td',{'class','tabev6'})[i].get_text())\n",
    "    latitude.append(list(zeroth_ratio_sibs[0]))\n",
    "    latitude_direction.append(list(zeroth_ratio_sibs[1]))\n",
    "    longitude.append(list(zeroth_ratio_sibs[2]))\n",
    "    longitude_direction.append(list(zeroth_ratio_sibs[3]))\n",
    "    magnitude.append(list(zeroth_ratio_sibs[6]))  \n",
    "    region.append(list(zeroth_ratio_sibs[7]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba14bee",
   "metadata": {},
   "source": [
    "Now I need to clean them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c488f681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['earthquake2023-01-10\\xa0\\xa0\\xa007:15:11.919min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa006:59:49.834min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa006:53:02.741min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa006:35:34.058min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa006:19:01.51hr 15min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa006:15:12.51hr 19min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa006:13:45.91hr 20min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa006:01:52.71hr 32min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa005:57:58.01hr 36min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa005:43:33.01hr 50min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa005:39:53.01hr 54min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa005:30:46.02hr 03min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa005:24:50.32hr 09min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa005:07:34.02hr 26min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa005:02:21.72hr 31min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa004:45:04.22hr 49min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa004:43:24.12hr 50min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa004:22:00.23hr 12min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa003:47:40.13hr 46min ago',\n",
       " 'earthquake2023-01-10\\xa0\\xa0\\xa003:43:47.93hr 50min ago']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bed0b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern_date = '(\\d{4}-\\d{2}-\\d{2})'\n",
    "pattern_time = '(\\d{2}:\\d{2}:\\d{2})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98ecd35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "date_c = []\n",
    "for i in range(0,20):\n",
    "    date.append(re.findall(pattern_date, date_time[i]))\n",
    "    date_c.append(date[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5519a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = []\n",
    "time_c = []\n",
    "for i in range(0,20):\n",
    "    time.append(re.findall(pattern_time, date_time[i]))\n",
    "    time_c.append(time[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30846c92",
   "metadata": {},
   "source": [
    "From below, there is \\xa0 strings in values, and values are saved as lists: for_ex[0][0] and for_ex[1][0], so I will clean them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e6f29",
   "metadata": {},
   "source": [
    "I will look for a shorter sloution later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c28b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(0,20):\n",
    "    res.append(latitude[i][0])\n",
    "    latitude_c = [sub.replace('\\xa0', '') for sub in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e55737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = []\n",
    "for i in range(0,20):\n",
    "    res2.append(latitude_direction[i][0])\n",
    "    latitude_direction_c = [sub.replace('\\xa0', '') for sub in res2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "181885d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = []\n",
    "for i in range(0,20):\n",
    "    res3.append(longitude[i][0])\n",
    "    longitude_c = [sub.replace('\\xa0', '') for sub in res3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01fb306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res4 = []\n",
    "for i in range(0,20):\n",
    "    res4.append(longitude_direction[i][0])\n",
    "    longitude_direction_c = [sub.replace('\\xa0', '') for sub in res4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e0a01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_c = []\n",
    "for i in range(0,20):\n",
    "    magnitude_c.append(magnitude[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "816b1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "res5 = []\n",
    "for i in range(0,20):\n",
    "    res5.append(region[i][0])\n",
    "    region_c = [sub.replace('\\xa0', '') for sub in res5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "653111df",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_earthquakes = pd.DataFrame({\"region\":region_c, \"date\":date_c, \"time\":time_c, \"latitude1\":latitude_c, \"lat_dir\":latitude_direction_c,\"longitude1\":longitude_c,\"long_dir\":longitude_direction_c,\"magnitude\":magnitude_c})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "731c25b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_earthquakes['latitude'] = latest_earthquakes['latitude1'] +' '+ latest_earthquakes['lat_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "587f6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_earthquakes['longitude'] = latest_earthquakes['longitude1'] +' '+ latest_earthquakes['long_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2b0c7bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_earthquakes=latest_earthquakes.drop(columns=['latitude1','lat_dir','longitude1','long_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21326ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_earthquakes = latest_earthquakes[['date', 'time', 'latitude','longitude','region','magnitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3735d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>region</th>\n",
       "      <th>magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>07:15:11</td>\n",
       "      <td>35.54 N</td>\n",
       "      <td>25.78 E</td>\n",
       "      <td>CRETE, GREECE</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>06:59:49</td>\n",
       "      <td>37.74 N</td>\n",
       "      <td>122.13 W</td>\n",
       "      <td>SAN FRANCISCO BAY AREA, CALIF.</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>06:53:02</td>\n",
       "      <td>17.97 N</td>\n",
       "      <td>66.87 W</td>\n",
       "      <td>PUERTO RICO</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>06:35:34</td>\n",
       "      <td>2.50 S</td>\n",
       "      <td>140.69 E</td>\n",
       "      <td>NEAR N COAST OF PAPUA, INDONESIA</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>06:19:01</td>\n",
       "      <td>18.13 N</td>\n",
       "      <td>67.24 W</td>\n",
       "      <td>MONA PASSAGE, PUERTO RICO</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>06:15:12</td>\n",
       "      <td>40.67 N</td>\n",
       "      <td>30.46 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>06:13:45</td>\n",
       "      <td>19.56 N</td>\n",
       "      <td>155.98 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>06:01:52</td>\n",
       "      <td>18.85 N</td>\n",
       "      <td>64.41 W</td>\n",
       "      <td>VIRGIN ISLANDS REGION</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>05:57:58</td>\n",
       "      <td>9.18 S</td>\n",
       "      <td>115.69 E</td>\n",
       "      <td>SOUTH OF BALI, INDONESIA</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>05:43:33</td>\n",
       "      <td>39.37 N</td>\n",
       "      <td>26.24 E</td>\n",
       "      <td>NEAR THE COAST OF WESTERN TURKEY</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>05:39:53</td>\n",
       "      <td>9.60 S</td>\n",
       "      <td>112.93 E</td>\n",
       "      <td>SOUTH OF JAVA, INDONESIA</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>05:30:46</td>\n",
       "      <td>22.48 S</td>\n",
       "      <td>68.80 W</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>05:24:50</td>\n",
       "      <td>14.97 S</td>\n",
       "      <td>167.28 E</td>\n",
       "      <td>VANUATU</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>05:07:34</td>\n",
       "      <td>8.28 N</td>\n",
       "      <td>82.83 W</td>\n",
       "      <td>PANAMA-COSTA RICA BORDER REGION</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>05:02:21</td>\n",
       "      <td>37.26 N</td>\n",
       "      <td>121.64 W</td>\n",
       "      <td>NORTHERN CALIFORNIA</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>04:45:04</td>\n",
       "      <td>17.99 N</td>\n",
       "      <td>66.89 W</td>\n",
       "      <td>PUERTO RICO</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>04:43:24</td>\n",
       "      <td>35.56 N</td>\n",
       "      <td>25.80 E</td>\n",
       "      <td>CRETE, GREECE</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>04:22:00</td>\n",
       "      <td>28.07 N</td>\n",
       "      <td>16.21 W</td>\n",
       "      <td>CANARY ISLANDS, SPAIN REGION</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>03:47:40</td>\n",
       "      <td>18.07 N</td>\n",
       "      <td>65.39 W</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>03:43:47</td>\n",
       "      <td>29.28 N</td>\n",
       "      <td>17.29 W</td>\n",
       "      <td>CANARY ISLANDS, SPAIN REGION</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date      time latitude longitude                            region  \\\n",
       "0   2023-01-10  07:15:11  35.54 N   25.78 E                     CRETE, GREECE   \n",
       "1   2023-01-10  06:59:49  37.74 N  122.13 W    SAN FRANCISCO BAY AREA, CALIF.   \n",
       "2   2023-01-10  06:53:02  17.97 N   66.87 W                       PUERTO RICO   \n",
       "3   2023-01-10  06:35:34   2.50 S  140.69 E  NEAR N COAST OF PAPUA, INDONESIA   \n",
       "4   2023-01-10  06:19:01  18.13 N   67.24 W         MONA PASSAGE, PUERTO RICO   \n",
       "5   2023-01-10  06:15:12  40.67 N   30.46 E                    WESTERN TURKEY   \n",
       "6   2023-01-10  06:13:45  19.56 N  155.98 W          ISLAND OF HAWAII, HAWAII   \n",
       "7   2023-01-10  06:01:52  18.85 N   64.41 W             VIRGIN ISLANDS REGION   \n",
       "8   2023-01-10  05:57:58   9.18 S  115.69 E          SOUTH OF BALI, INDONESIA   \n",
       "9   2023-01-10  05:43:33  39.37 N   26.24 E  NEAR THE COAST OF WESTERN TURKEY   \n",
       "10  2023-01-10  05:39:53   9.60 S  112.93 E          SOUTH OF JAVA, INDONESIA   \n",
       "11  2023-01-10  05:30:46  22.48 S   68.80 W                ANTOFAGASTA, CHILE   \n",
       "12  2023-01-10  05:24:50  14.97 S  167.28 E                           VANUATU   \n",
       "13  2023-01-10  05:07:34   8.28 N   82.83 W   PANAMA-COSTA RICA BORDER REGION   \n",
       "14  2023-01-10  05:02:21  37.26 N  121.64 W               NORTHERN CALIFORNIA   \n",
       "15  2023-01-10  04:45:04  17.99 N   66.89 W                       PUERTO RICO   \n",
       "16  2023-01-10  04:43:24  35.56 N   25.80 E                     CRETE, GREECE   \n",
       "17  2023-01-10  04:22:00  28.07 N   16.21 W      CANARY ISLANDS, SPAIN REGION   \n",
       "18  2023-01-10  03:47:40  18.07 N   65.39 W                PUERTO RICO REGION   \n",
       "19  2023-01-10  03:43:47  29.28 N   17.29 W      CANARY ISLANDS, SPAIN REGION   \n",
       "\n",
       "   magnitude  \n",
       "0        2.1  \n",
       "1        2.7  \n",
       "2        2.1  \n",
       "3        3.8  \n",
       "4        2.6  \n",
       "5        3.4  \n",
       "6        2.8  \n",
       "7        3.8  \n",
       "8        3.5  \n",
       "9        2.3  \n",
       "10       3.4  \n",
       "11       2.7  \n",
       "12       5.7  \n",
       "13       3.2  \n",
       "14       2.2  \n",
       "15       3.5  \n",
       "16       3.0  \n",
       "17       1.9  \n",
       "18       2.3  \n",
       "19       2.3  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ac5f2",
   "metadata": {},
   "source": [
    "Short-correct way for earthquakes-I will look into that later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f802f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #initialize empty lists\n",
    "# date_time = []\n",
    "# latitude = []\n",
    "# longitude = []\n",
    "# depth = []\n",
    "# magnitude = []\n",
    "# region_name = []\n",
    "# latitude_SN = []\n",
    "# longitude_EW = []\n",
    "\n",
    "# # define the number of iterations of our for loop\n",
    "# # by checking how many elements are in the retrieved result set\n",
    "# # (this is equivalent but more robust than just explicitly defining 250 iterations)\n",
    "# num_iter = len(soup3.select('td.tabev5'))\n",
    "\n",
    "# date_time_list = soup3.select('b > a')\n",
    "# region_name_list = soup3.select('td.tb_region')\n",
    "# depth_list = soup3.select('td.tabev3')\n",
    "\n",
    "# latitude_list = soup3.select('td.tabev1')\n",
    "# longitude_list = soup3.select('td.tabev1')\n",
    "\n",
    "# latitude_SN_list = soup3.select('td.tabev2')\n",
    "# longitude_EW_list = soup3.select('td.tabev2')\n",
    "# magnitude_list = soup3.select('td.tabev2')\n",
    "\n",
    "# # iterate through the result set and retrieve all the data\n",
    "# for i in range(num_iter):\n",
    "#     date_time.append(date_time_list[i].get_text())\n",
    "#     depth.append(depth_list[i].get_text())\n",
    "#     region_name.append(region_name_list[i].get_text())\n",
    "    \n",
    "# for i in range(0,len(soup3.select('td.tabev5'))*2,2):\n",
    "#     latitude.append(latitude_list[i].get_text())\n",
    "#     longitude.append(longitude_list[i+1].get_text())\n",
    "\n",
    "# for i in range(0,len(soup3.select('td.tabev5'))*3,3):\n",
    "#     latitude_SN.append(latitude_SN_list[i].get_text())\n",
    "#     longitude_EW.append(longitude_EW_list[i+1].get_text())\n",
    "#     magnitude.append(magnitude_list[i+2].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "959da912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # each list becomes a column\n",
    "# top_20_earthquakes = pd.DataFrame({\"date & time\":date_time,\n",
    "#                        \"latitude degrees\":latitude,\n",
    "#                        \"latitude SN\":latitude_SN,\n",
    "#                        \"longitude degrees\":longitude,\n",
    "#                        \"longitude EW\":longitude_EW,\n",
    "#                        \"depth\":depth,\n",
    "#                        \"magnitude\":magnitude,\n",
    "#                        \"region\":region_name,\n",
    "#                       })\n",
    "# top_20_earthquakes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db590bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7a63dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
